{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Generative AI and Hands-On with Variational Autoencoders (VAEs)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand what Generative AI is and its real-world applications.\n",
    "- Compare generative models to traditional deep learning approaches.\n",
    "- Explain autoencoders, latent spaces, and the concept behind VAEs.\n",
    "- Gain practical experience by exploring latent space interpolation using a VAE on CelebA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Prerequisites\n",
    "\n",
    "Before running this notebook locally, make sure to set up your environment:\n",
    "\n",
    "```bash\n",
    "    # Create a virtual environment\n",
    "    python -m venv genai-env\n",
    "    source genai-env/bin/activate  # Use `genai-env\\\\Scripts\\\\activate` on Windows\n",
    "\n",
    "    # Install required packages\n",
    "    pip install torch torchvision matplotlib numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative AI vs Traditional approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Traditional DL** Focus on classification, regression, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in image classification, given an image $x$ classify if it is a cat or dog.\n",
    "\n",
    "<img src=\"./images/classification-object-detection.png\" width=\"600\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "*Image Source: [ambolt.io](https://ambolt.io/en/image-classification-and-object-detection/)*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generative AI** models on the other hand learn the underlying data distribution  $p^{*}(x)$ so that we can generate new, synthetic data similar to the training examples by sampling from $ x \\sim p^{*}(x) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of an AI generated image of a kitten.\n",
    "\n",
    "<img src=\"./images/AI_generated_kitten.webp\" width=\"300\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "*Image Source: [Dall-E](https://openai.com/dall-e-3)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Spaces & Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Latent space** is a simplified, compressed representation of data where similar items are grouped closer together, capturing hidden patterns or features that define their structure. It's like a map that organizes complex data into meaningful, lower-dimensional coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the latent space of the mnist dataset\n",
    "<img src=\"./images/aae_latent.png\" width=\"500\" style=\"display: block; margin: auto;\">\n",
    "*Image Source: [https://github.com/greentfrapp/keras-aae](https://github.com/greentfrapp/keras-aae)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autoencoders** capture the underlying latent space.\n",
    "- **Autoencoder Architecture:**  \n",
    "  - **Encoder:** Compresses the input data into a lower-dimensional latent representation.\n",
    "  - **Decoder:** Reconstructs the input from the latent representation.\n",
    "- **Loss Function:** Typically, a reconstruction loss (e.g., Mean Squared Error) is used to train the autoencoder.\n",
    "- **Latent Space:**  \n",
    "  - It represents the compressed, learned features.\n",
    "  - Enables operations like interpolation between data points and semantic modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/variational-autoencoder-neural-network.png\" width=\"900\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "*Image Source: [ibm.com](https://www.ibm.com/de-de/think/topics/variational-autoencoder)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Autoencoder in PyTorch\n",
    "\n",
    "In this following, we’ll build and train a simple autoencoder on the MNIST dataset using PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhussam-alafandi\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hussam/Documents/Hochschule/Generative_AI/01/wandb/run-20250324_143528-f81c7dgf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hussam-alafandi/mnist-autoencoder/runs/f81c7dgf' target=\"_blank\">comfy-wind-4</a></strong> to <a href='https://wandb.ai/hussam-alafandi/mnist-autoencoder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hussam-alafandi/mnist-autoencoder' target=\"_blank\">https://wandb.ai/hussam-alafandi/mnist-autoencoder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hussam-alafandi/mnist-autoencoder/runs/f81c7dgf' target=\"_blank\">https://wandb.ai/hussam-alafandi/mnist-autoencoder/runs/f81c7dgf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hussam-alafandi/mnist-autoencoder/runs/f81c7dgf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x71fd9a5cda90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()  # only needed once per machine/session\n",
    "\n",
    "config = {\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"architecture\": \"Autoencoder\",\n",
    "    \"dataset\": \"MNIST\",\n",
    "    \"latent_dim\": 10,\n",
    "    \"train_split\": 0.8\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    project=\"mnist-autoencoder\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST Dataset\n",
    "\n",
    "We use the torchvision `MNIST` dataset class and apply a basic transformation to convert images to tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = ToTensor()\n",
    "\n",
    "train_data = MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "val_data = MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Autoencoder Model\n",
    "\n",
    "The autoencoder has two main parts:  \n",
    "- **Encoder:** Compresses the 28x28 image into a smaller latent vector  \n",
    "- **Decoder:** Reconstructs the image from the latent vector  \n",
    "We use linear layers with ReLU activations and end the decoder with a sigmoid activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, wandb.config.latent_dim),\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(wandb.config.latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_flat = x.flatten(start_dim=1)\n",
    "\n",
    "        encoded = self.encoder(x_flat)\n",
    "        decoded = self.decoder(encoded)\n",
    "\n",
    "        return decoded.reshape_as(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Model, Loss Function, and Optimizer\n",
    "\n",
    "We’ll use:\n",
    "- **MSELoss** to measure reconstruction error.\n",
    "- **Adam** optimizer for training.\n",
    "Make sure to define the device (CPU or GPU) before using `.to(device)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "autoencoder = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=wandb.config.learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Autoencoder\n",
    "\n",
    "Loop through the dataset for multiple epochs.  \n",
    "For each batch:\n",
    "- Forward pass through the model  \n",
    "- Compute the loss  \n",
    "- Backpropagate and update weights  \n",
    "- Track the loss for monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def log_reconstruction_images(model, dataloader, device, n=6, tag=\"Reconstruction\"):\n",
    "    model.eval()\n",
    "    images, _ = next(iter(dataloader))\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "\n",
    "    # Plot original and reconstructed\n",
    "    fig, axs = plt.subplots(2, n, figsize=(n * 2, 4))\n",
    "    for i in range(n):\n",
    "        axs[0, i].imshow(images[i].squeeze().cpu(), cmap=\"gray\")\n",
    "        axs[0, i].set_title(\"Original\")\n",
    "        axs[0, i].axis(\"off\")\n",
    "        axs[1, i].imshow(outputs[i].squeeze().cpu(), cmap=\"gray\")\n",
    "        axs[1, i].set_title(\"Reconstructed\")\n",
    "        axs[1, i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Log to wandb\n",
    "    wandb.log({tag: wandb.Image(fig)})\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Train Loss: 0.0499, Val Loss: 0.0322\n",
      "Epoch 2/40, Train Loss: 0.0279, Val Loss: 0.0242\n",
      "Epoch 3/40, Train Loss: 0.0231, Val Loss: 0.0215\n",
      "Epoch 4/40, Train Loss: 0.0212, Val Loss: 0.0203\n",
      "Epoch 5/40, Train Loss: 0.0202, Val Loss: 0.0193\n",
      "Epoch 6/40, Train Loss: 0.0194, Val Loss: 0.0189\n",
      "Epoch 7/40, Train Loss: 0.0188, Val Loss: 0.0184\n",
      "Epoch 8/40, Train Loss: 0.0184, Val Loss: 0.0179\n",
      "Epoch 9/40, Train Loss: 0.0180, Val Loss: 0.0176\n",
      "Epoch 10/40, Train Loss: 0.0177, Val Loss: 0.0175\n",
      "Epoch 11/40, Train Loss: 0.0174, Val Loss: 0.0172\n",
      "Epoch 12/40, Train Loss: 0.0172, Val Loss: 0.0169\n",
      "Epoch 13/40, Train Loss: 0.0170, Val Loss: 0.0167\n",
      "Epoch 14/40, Train Loss: 0.0168, Val Loss: 0.0167\n",
      "Epoch 15/40, Train Loss: 0.0166, Val Loss: 0.0168\n",
      "Epoch 16/40, Train Loss: 0.0165, Val Loss: 0.0164\n",
      "Epoch 17/40, Train Loss: 0.0163, Val Loss: 0.0162\n",
      "Epoch 18/40, Train Loss: 0.0162, Val Loss: 0.0161\n",
      "Epoch 19/40, Train Loss: 0.0161, Val Loss: 0.0160\n",
      "Epoch 20/40, Train Loss: 0.0160, Val Loss: 0.0160\n",
      "Epoch 21/40, Train Loss: 0.0159, Val Loss: 0.0160\n",
      "Epoch 22/40, Train Loss: 0.0158, Val Loss: 0.0157\n",
      "Epoch 23/40, Train Loss: 0.0157, Val Loss: 0.0157\n",
      "Epoch 24/40, Train Loss: 0.0156, Val Loss: 0.0157\n",
      "Epoch 25/40, Train Loss: 0.0155, Val Loss: 0.0156\n",
      "Epoch 26/40, Train Loss: 0.0155, Val Loss: 0.0155\n",
      "Epoch 27/40, Train Loss: 0.0154, Val Loss: 0.0155\n",
      "Epoch 28/40, Train Loss: 0.0153, Val Loss: 0.0155\n",
      "Epoch 29/40, Train Loss: 0.0153, Val Loss: 0.0154\n",
      "Epoch 30/40, Train Loss: 0.0152, Val Loss: 0.0153\n",
      "Epoch 31/40, Train Loss: 0.0152, Val Loss: 0.0153\n",
      "Epoch 32/40, Train Loss: 0.0151, Val Loss: 0.0153\n",
      "Epoch 33/40, Train Loss: 0.0150, Val Loss: 0.0152\n",
      "Epoch 34/40, Train Loss: 0.0150, Val Loss: 0.0151\n",
      "Epoch 35/40, Train Loss: 0.0150, Val Loss: 0.0151\n",
      "Epoch 36/40, Train Loss: 0.0149, Val Loss: 0.0151\n",
      "Epoch 37/40, Train Loss: 0.0149, Val Loss: 0.0150\n",
      "Epoch 38/40, Train Loss: 0.0148, Val Loss: 0.0150\n",
      "Epoch 39/40, Train Loss: 0.0148, Val Loss: 0.0150\n",
      "Epoch 40/40, Train Loss: 0.0148, Val Loss: 0.0150\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>40</td></tr><tr><td>train_loss</td><td>0.01475</td></tr><tr><td>val_loss</td><td>0.015</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comfy-wind-4</strong> at: <a href='https://wandb.ai/hussam-alafandi/mnist-autoencoder/runs/f81c7dgf' target=\"_blank\">https://wandb.ai/hussam-alafandi/mnist-autoencoder/runs/f81c7dgf</a><br> View project at: <a href='https://wandb.ai/hussam-alafandi/mnist-autoencoder' target=\"_blank\">https://wandb.ai/hussam-alafandi/mnist-autoencoder</a><br>Synced 5 W&B file(s), 8 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250324_143528-f81c7dgf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(wandb.config.epochs):\n",
    "    autoencoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_val_loss = evaluate_model(autoencoder, val_loader, criterion)\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"val_loss\": avg_val_loss,\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{wandb.config.epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Every 5 epochs: log images\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        log_reconstruction_images(autoencoder, val_loader, device)\n",
    "\n",
    "wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Trained Model\n",
    "\n",
    "save the trained model for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Save the trained model (optional)\n",
    "model_dir = \"autoencoder-mnist\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "torch.save(autoencoder.state_dict(), Path(model_dir) / \"autoencoder_mnist.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders (VAEs)\n",
    "\n",
    "**Motivation for VAEs:**\n",
    "- **Regularization:** VAEs enforce a continuous, smooth latent space by modeling the encoder output as a probability distribution.\n",
    "- **Key Components:**  \n",
    "  - **KL Divergence:** A regularization term that encourages the latent distribution to be close to a prior (usually a standard Gaussian).\n",
    "  - **Reparameterization Trick:** Allows backpropagation through stochastic nodes.\n",
    "- **Benefits:**  \n",
    "  - Smooth interpolation between data points.\n",
    "  - More meaningful manipulations in the latent space.\n",
    "\n",
    "*Concept Check:* Think about how changing parts of a latent vector might modify attributes (e.g., facial expression, pose) in generated images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Hands-On Exercise – VAE with CelebA\n",
    "\n",
    "In this section, we’ll work with a VAE on the CelebA dataset. The goals are:\n",
    "- Load and preprocess the CelebA dataset.\n",
    "- Define and (optionally) load a pretrained VAE model.\n",
    "- Encode images to obtain their latent representations.\n",
    "- Interpolate between latent vectors and decode the results to see how the generated images change.\n",
    "\n",
    "**Note:** Training a VAE on CelebA from scratch is computationally intensive. For this demonstration, you can either:\n",
    "- Use a pretrained model checkpoint (if available), or\n",
    "- Train a simplified model on a subset of the dataset for a few epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Library Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data Preparation - Load CelebA (subset)\n",
    "\n",
    "# Define transformations for the CelebA dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(178),\n",
    "    transforms.Resize(64),  # Resize to a smaller size for quick experimentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Download CelebA dataset (this may take a few minutes; you can also set download=False if already downloaded)\n",
    "celeba_data = datasets.CelebA(root='./data', split='train', transform=transform, download=True)\n",
    "\n",
    "# Create a DataLoader with a small subset (e.g., first 500 images)\n",
    "subset_size = 500\n",
    "subset_indices = list(range(subset_size))\n",
    "celeba_subset = torch.utils.data.Subset(celeba_data, subset_indices)\n",
    "data_loader = DataLoader(celeba_subset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(\"Number of images in subset:\", len(celeba_subset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Defining a Simple VAE Model\n",
    "\n",
    "Below is a simplified convolutional VAE model. In a real-world course, you might provide a more refined architecture or a pretrained checkpoint. For this exercise, we define the model and show how to perform encoding and decoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define the VAE Model\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),  # 64 -> 32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 32 -> 16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # 16 -> 8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), # 8 -> 4\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256*4*4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256*4*4, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, 256*4*4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # 4 -> 8\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 8 -> 16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),   # 16 -> 32\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),    # 32 -> 64\n",
    "            nn.Tanh()  # Output values in [-1, 1] due to normalization\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x_enc = self.encoder(x)\n",
    "        x_enc = x_enc.view(x_enc.size(0), -1)\n",
    "        mu = self.fc_mu(x_enc)\n",
    "        logvar = self.fc_logvar(x_enc)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x_dec = self.decoder_input(z)\n",
    "        x_dec = x_dec.view(-1, 256, 4, 4)\n",
    "        x_recon = self.decoder(x_dec)\n",
    "        return x_recon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# Initialize the VAE model and move it to the device\n",
    "latent_dim = 32\n",
    "model = VAE(latent_dim=latent_dim).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: (Optional) Training the VAE\n",
    "\n",
    "Training a VAE can be time-consuming. For demonstration, you can either load a pretrained model or train for a few epochs on the subset. Below is a simple training loop that you can run if you wish to see the model learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Training Setup (Optional)\n",
    "\n",
    "# Loss function components: Reconstruction Loss and KL Divergence\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # Reconstruction loss (MSE)\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    # KL Divergence\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 3  # For demonstration; increase for better results\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Average Loss: {train_loss/len(celeba_subset):.4f}\")\n",
    "\n",
    "# After training, you can save the model if desired:\n",
    "# torch.save(model.state_dict(), \"vae_celeba.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Latent Space Manipulation & Interpolation\n",
    "\n",
    "Now that our VAE is (optionally) trained, let’s demonstrate latent space interpolation. We will:\n",
    "1. Select two images from the dataset.\n",
    "2. Encode them to obtain their latent vectors.\n",
    "3. Linearly interpolate between the two latent vectors.\n",
    "4. Decode each interpolated vector to visualize the transition between the two images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Latent Space Interpolation Demo\n",
    "\n",
    "def interpolate(z1, z2, num_steps=8):\n",
    "    \"\"\"Generate a series of interpolated latent vectors between z1 and z2.\"\"\"\n",
    "    ratios = np.linspace(0, 1, num_steps)\n",
    "    interpolated = [ (1 - r) * z1 + r * z2 for r in ratios ]\n",
    "    return torch.stack(interpolated)\n",
    "\n",
    "# Get two images from the dataset\n",
    "data_iter = iter(data_loader)\n",
    "images, _ = next(data_iter)\n",
    "img1 = images[0].unsqueeze(0).to(device)\n",
    "img2 = images[1].unsqueeze(0).to(device)\n",
    "\n",
    "# Encode images\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mu1, _ = model.encode(img1)\n",
    "    mu2, _ = model.encode(img2)\n",
    "\n",
    "# Interpolate in the latent space\n",
    "num_steps = 8\n",
    "interpolated_z = interpolate(mu1, mu2, num_steps=num_steps)\n",
    "\n",
    "# Decode the interpolated latent vectors\n",
    "with torch.no_grad():\n",
    "    decoded_imgs = model.decode(interpolated_z).cpu()\n",
    "\n",
    "# Plot the interpolation results\n",
    "fig, axes = plt.subplots(1, num_steps, figsize=(20, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    # Denormalize image from [-1,1] to [0,1]\n",
    "    img = (decoded_imgs[i].permute(1, 2, 0).numpy() + 1) / 2.0\n",
    "    ax.imshow(np.clip(img, 0, 1))\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Latent Space Interpolation between Two CelebA Images\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Discussion & Next Steps\n",
    "\n",
    "- **Discussion Points:**  \n",
    "  - What do you observe in the interpolation results?  \n",
    "  - How does the latent space capture semantic features of faces?\n",
    "  - How might you extend this approach (e.g., by conditioning on attributes)?\n",
    "\n",
    "- **Further Experiments:**  \n",
    "  - Try random sampling from the latent space.\n",
    "  - Modify specific dimensions in the latent vector to observe changes in output attributes.\n",
    "  - Explore different architectures or pretrained models for improved image quality.\n",
    "\n",
    "---\n",
    "\n",
    "## Homework / Reflection\n",
    "\n",
    "- **Write a short summary:** Explain in your own words what a VAE is and how the latent space allows for creative manipulation of images.\n",
    "- **Experiment:** If you have extra time, modify the interpolation function to explore non-linear paths in latent space.\n",
    "\n",
    "---\n",
    "\n",
    "**Recommended Reading & Resources:**\n",
    "- [A Beginner’s Guide to VAEs (arXiv)](https://arxiv.org/abs/1906.02691)\n",
    "- [PyTorch VAE Tutorial](https://github.com/pytorch/examples/tree/main/vae)\n",
    "- [Understanding Latent Space](https://towardsdatascience.com/understanding-latent-space-in-machine-learning-4de5473c44f)\n",
    "\n",
    "---\n",
    "\n",
    "*End of Day 1 Notebook*\n",
    "\n",
    "Feel free to ask if you need any modifications or further details on any section!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
