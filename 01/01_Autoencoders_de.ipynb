{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/hussamalafandi/Generative_AI/blob/main/01/01_Autoencoders_de.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" style=\"display: inline; margin: 0 auto; float: right;\"\\>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "**Autoencoder** sind neuronale Netze, die darauf ausgelegt sind, kompakte und bedeutungsvolle Repräsentationen von Daten zu erlernen, indem sie die Eingabedaten komprimieren (kodieren) und anschließend wieder rekonstruieren (dekodieren).\n",
    "\n",
    "- **Architektur:**  \n",
    "  - **Encoder:** Reduziert die Eingabe auf eine niedrigdimensionale latente Repräsentation.  \n",
    "  - **Decoder:** Rekonstruiert aus der latenten Repräsentation wieder die Originaleingabe.  \n",
    "- **Verlustfunktion:** Misst den Unterschied zwischen Eingabe und Rekonstruktion, typischerweise mit dem mittleren quadratischen Fehler (MSE).  \n",
    "- **Latenter Raum:**  \n",
    "  - Enthält verdichtete, bedeutungsvolle Merkmale der Daten.  \n",
    "  - Ermöglicht Aufgaben wie Interpolation oder semantische Transformationen.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/b6/f4/variational-autoencoder-neural-network.png\" width=\"900\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "*Image Source: [ibm.com](https://www.ibm.com/de-de/think/topics/variational-autoencoder)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Autoencoder mit PyTorch erstellen\n",
    "\n",
    "In diesem Abschnitt erstellen und trainieren wir einen einfachen Autoencoder auf dem MNIST-Datensatz mit PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir beginnen mit dem Import der benötigten Bibliotheken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases\n",
    "Zur Nachverfolgung unserer Experimente und zur Visualisierung des Trainingsverlaufs nutzen wir [WandB](https://wandb.ai). Dafür wird ein kostenloser Account benötigt. Installiere WandB mit `pip install wandb` und initialisiere es wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()  # only needed once per machine/session\n",
    "\n",
    "config = {\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"architecture\": \"Autoencoder\",\n",
    "    \"dataset\": \"MNIST\",\n",
    "    \"latent_dim\": 10\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lade den MNIST-Datensatz\n",
    "\n",
    "Wir verwenden die `MNIST`-Klasse von torchvision und wandeln die Bilder in Tensoren um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = ToTensor() # Convert images to PyTorch tensors and normalize to [0, 1]\n",
    "\n",
    "train_data = MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "val_data = MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=config[\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definiere das Autoencoder-Modell\n",
    "\n",
    "Der Autoencoder besteht aus zwei Hauptkomponenten:  \n",
    "- **Encoder:** Komprimiert das 28x28-Bild zu einem latenten Vektor.  \n",
    "- **Decoder:** Rekonstruiert das Bild aus diesem Vektor.  \n",
    "Wir verwenden lineare Schichten mit ReLU-Aktivierungen und schließen den Decoder mit einer Sigmoid-Aktivierung ab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, config[\"latent_dim\"])\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(config[\"latent_dim\"], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28 * 28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_flat = x.flatten(start_dim=1)\n",
    "\n",
    "        encoded = self.encoder(x_flat)\n",
    "        decoded = self.decoder(encoded)\n",
    "\n",
    "        return decoded.reshape_as(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisiere Modell, Verlustfunktion und Optimierer\n",
    "\n",
    "Wir verwenden:\n",
    "- **MSELoss** zur Berechnung des Rekonstruktionsfehlers zwischen Eingabe $x$ und Ausgang $\\hat{x}$.  \n",
    "  $$\n",
    "  \\text{MSELoss} = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\hat{x}_i)^2\n",
    "  $$\n",
    "- **Adam** als Optimierer, da dieser adaptive Lernraten für jede Gewichtung berechnet und sich gut für dieses Szenario eignet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## Bewertungsmethoden erstellen\n",
    "\n",
    "Wir bewerten die Leistung des Autoencoders, indem wir rekonstruierte Bilder visualisieren und sie mit den Originalbildern vergleichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_reconstruction_images(model, dataloader, device, n=6):\n",
    "    model.eval()\n",
    "    images, _ = next(iter(dataloader))\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "\n",
    "    # Plot original and reconstructed\n",
    "    fig, axs = plt.subplots(2, n, figsize=(n * 2, 4))\n",
    "    for i in range(n):\n",
    "        axs[0, i].imshow(images[i].squeeze().cpu(), cmap=\"gray\")\n",
    "        axs[0, i].set_title(\"Original\")\n",
    "        axs[0, i].axis(\"off\")\n",
    "        axs[1, i].imshow(outputs[i].squeeze().cpu(), cmap=\"gray\")\n",
    "        axs[1, i].set_title(\"Reconstructed\")\n",
    "        axs[1, i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder trainieren\n",
    "\n",
    "Der Trainingsloop läuft über mehrere Epochen. Für jedes Mini-Batch gilt:  \n",
    "- Durchlauf durch das Modell (Forward-Pass)  \n",
    "- Berechnung des Fehlers (Loss)  \n",
    "- Rückpropagation und Gewichtsaktualisierung  \n",
    "- Protokollieren der Fehlerwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    wandb.init(\n",
    "        project=\"mnist-autoencoder\",\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, images)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_val_loss = evaluate_model(model, val_loader, criterion)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{wandb.config.epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Every 5 epochs: log images\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            fig = plot_reconstruction_images(model, val_loader, device, n=6)\n",
    "            wandb.log({\"reconstruction_images\": wandb.Image(fig)})\n",
    "\n",
    "            fig.savefig(f'reconstruction_epoch_{epoch + 1}.png')\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    return model, avg_train_loss, avg_val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell speichern oder laden\n",
    "\n",
    "Wenn bereits ein trainiertes Modell gespeichert wurde, wird es geladen. Ansonsten wird das Modell trainiert und danach gespeichert. Das spart Rechenzeit und beschleunigt spätere Experimente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_dir = \"./models/autoencoder-mnist\"\n",
    "\n",
    "# Check if the model file exists\n",
    "model_path = Path(model_dir) / f\"autoencoder_mnist_latent_{config['latent_dim']}_epochs_{config['epochs']}.pth\"\n",
    "\n",
    "if model_path.exists():\n",
    "    print(\"Model file found. Loading the model...\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    fig = plot_reconstruction_images(model, val_loader, device, n=6)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Model file not found. Training the model...\")\n",
    "    # Train the model\n",
    "    model, avg_train_loss, avg_val_loss = train_model(model, train_loader, val_loader, criterion, optimizer, config[\"epochs\"])\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latenten Raum visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_latent_space(autoencoder, dataloader, device):\n",
    "    autoencoder.eval()\n",
    "    latent_vectors = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device)\n",
    "            encoded = autoencoder.encoder(images.view(images.size(0), -1))\n",
    "            latent_vectors.append(encoded.cpu())\n",
    "            labels.append(targets)\n",
    "\n",
    "    latent_vectors = torch.cat(latent_vectors)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    # Apply PCA to reduce dimensions to 2\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_latent_vectors = pca.fit_transform(latent_vectors)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(reduced_latent_vectors[:, 0], reduced_latent_vectors[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "    plt.colorbar(scatter, label=\"Digit Label\")\n",
    "    plt.title(\"Latent Space Visualization (PCA Reduced)\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the latent space using the validation data\n",
    "visualize_latent_space(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können neue Bilder erzeugen, indem wir zufällige Punkte im latenten Raum samplen und durch den Decoder schicken. Allerdings erscheinen die erzeugten Bilder oft wie Rauschen, weil der latente Raum nicht strukturiert ist. Das zeigt die Notwendigkeit für ein Modell wie den **Variational Autoencoder**, der die Struktur im latenten Raum erzwingt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_latents(model, num_samples=8, latent_dim=2, device='cuda'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Sample from a standard normal distribution\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        generated = model.decoder(z).cpu()\n",
    "\n",
    "    # Plot the generated images\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 2, 2))\n",
    "    for i in range(num_samples):\n",
    "        axes[i].imshow(generated[i].view(28, 28), cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    plt.suptitle(\"Random Samples from Vanilla Autoencoder Latent Space\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sample_random_latents(model, latent_dim=config['latent_dim'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Übung\n",
    "\n",
    "Modifiziere den Autoencoder so, dass er Convolutional Layers statt Linear Layers verwendet. Vergleiche die Leistung beider Modelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Übung\n",
    "\n",
    "Teste den klassischen Autoencoder und die CNN-Variante mit dem Fashion-MNIST-Datensatz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE)\n",
    "\n",
    "**Motivation für VAEs:**\n",
    "- **Regulierung:** VAEs sorgen durch probabilistische Kodierung für einen glatten, kontinuierlichen latenten Raum.\n",
    "- **Wichtige Komponenten:**\n",
    "  - **KL-Divergenz:** Reguliert die Verteilung im latenten Raum, damit sie der Standardnormalverteilung ähnelt.\n",
    "  - **Reparametrisierungstrick:** Ermöglicht Backpropagation trotz Zufallskomponenten.\n",
    "- **Vorteile:**\n",
    "  - Bessere Interpolation zwischen Datenpunkten.\n",
    "  - Bedeutungsvolle Manipulationen im latenten Raum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*r1R0cxCnErWgE0P4Q-hI0Q.jpeg\" width=\"900\" style=\"display: block; margin: auto;\">\n",
    "\n",
    "*Image Source: [medium.com](https://medium.com/geekculture/variational-autoencoder-vae-9b8ce5475f68)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE mit CelebA\n",
    "\n",
    "In diesem Abschnitt arbeiten wir mit einem VAE auf dem CelebA-Datensatz. Ziel ist es:\n",
    "- CelebA laden und vorverarbeiten\n",
    "- Ein VAE-Modell definieren (oder laden)\n",
    "- Bilder encodieren und latente Repräsentationen extrahieren\n",
    "- Zwischen latenten Vektoren interpolieren und die Übergänge visualisieren\n",
    "\n",
    "**Hinweis:** Das vollständige Training eines VAE auf CelebA ist rechenintensiv. Für diese Demonstration:\n",
    "- Verwende ein vortrainiertes Modell, oder\n",
    "- Trainiere ein vereinfachtes Modell auf einem Teil des Datensatzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Check device\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"No GPU available. Please run on a machine with a GPU.\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "vae_config = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"architecture\": \"VAE\",\n",
    "    \"dataset\": \"CelebA\",\n",
    "    \"latent_dim\": 200,\n",
    "    \"subset_size\": 80000\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datenvorbereitung – Lade CelebA (Teilmenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for the CelebA dataset\n",
    "celeba_transform = transforms.Compose([\n",
    "    transforms.CenterCrop(178),\n",
    "    transforms.Resize(64),  # Resize to a smaller size for quick experimentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the CelebA dataset\n",
    "celeba_train_dataset = datasets.CelebA(root='./data', split='train', transform=celeba_transform, download=True)\n",
    "celeba_val_dataset = datasets.CelebA(root='./data', split='valid', transform=celeba_transform, download=True)\n",
    "\n",
    "# Subset the CelebA dataset for quick experimentation\n",
    "train_subset_size = vae_config[\"subset_size\"]  # Use a smaller subset of the training dataset\n",
    "train_subset_indices = list(range(train_subset_size))\n",
    "celeba_train_subset = torch.utils.data.Subset(celeba_train_dataset, train_subset_indices)\n",
    "\n",
    "val_subset_size = 10000  # Use a smaller subset of the validation dataset\n",
    "val_subset_indices = list(range(val_subset_size))\n",
    "celeba_val_subset = torch.utils.data.Subset(celeba_val_dataset, val_subset_indices)\n",
    "\n",
    "# Create DataLoaders for training and validation subsets\n",
    "celeba_train_loader = DataLoader(celeba_train_subset, batch_size=vae_config[\"batch_size\"], shuffle=True)\n",
    "celeba_val_loader = DataLoader(celeba_val_subset, batch_size=vae_config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(\"Number of images in training subset:\", len(celeba_train_subset))\n",
    "print(\"Number of images in validation subset:\", len(celeba_val_subset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einfaches VAE-Modell definieren\n",
    "\n",
    "Im Folgenden definieren wir ein vereinfachtes Convolutional VAE. Für komplexere Aufgaben könnte ein leistungsfähigeres Modell oder ein Checkpoint bereitgestellt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),  # 64 -> 32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 32 -> 16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # 16 -> 8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1), # 8 -> 4\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(256*4*4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256*4*4, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, 256*4*4)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # 4 -> 8\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 8 -> 16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),   # 16 -> 32\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),    # 32 -> 64\n",
    "            nn.Tanh()  # Output values in [-1, 1] due to normalization\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x_enc = self.encoder(x)\n",
    "        x_enc = torch.flatten(x_enc, start_dim=1)\n",
    "\n",
    "        mu = self.fc_mu(x_enc)\n",
    "        logvar = self.fc_logvar(x_enc)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x_dec = self.decoder_input(z)\n",
    "        x_dec = x_dec.reshape(-1, 256, 4, 4)\n",
    "        x_recon = self.decoder(x_dec)\n",
    "        return x_recon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# Initialize the VAE model and move it to the device\n",
    "latent_dim = vae_config[\"latent_dim\"]\n",
    "model = VAE(latent_dim=latent_dim).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verlustfunktion im VAE\n",
    "\n",
    "Die Verlustfunktion eines Variational Autoencoder kombiniert zwei Komponenten:\n",
    "\n",
    "1. **Rekonstruktionsverlust:** Misst, wie gut das Modell die Eingabe rekonstruiert (z. B. mit MSE).\n",
    "    - In this notebook, the reconstruction loss is calculated using Mean Squared Error (MSE):\n",
    "      $$\n",
    "      \\text{Reconstruction Loss} = \\sum_{i=1}^{N} (x_i - \\hat{x}_i)^2\n",
    "      $$\n",
    "      where $x_i$ is the original input, and $\\hat{x}_i$ is the reconstructed output.\n",
    "\n",
    "2. **KL-Divergenz:** Reguliert den latenten Raum, damit die Verteilung dem Standardnormalverhalten ähnelt.\n",
    "\n",
    "### Gesamter Verlust:\n",
    "$$\n",
    "\\text{VAE Loss} = \\text{Reconstruction Loss} + \\text{KL Divergence}\n",
    "$$\n",
    "\n",
    "This combined loss ensures that the VAE learns to reconstruct the input data while maintaining a smooth and continuous latent space, enabling meaningful interpolations and manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function components: Reconstruction Loss and KL Divergence\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # Reconstruction loss (MSE)\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    # KL Divergence\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_loss, recon_loss, kl_loss\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=vae_config[\"learning_rate\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bewertungsmethoden\n",
    "\n",
    "Hier definieren wir Funktionen zur Bewertung der Modellleistung, einschließlich Verlustberechnung und Visualisierung von Rekonstruktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vae(model, dataloader, device):\n",
    "    \"\"\"Evaluate the VAE model on a given dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_kl_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            recon_images, mu, logvar = model(images)\n",
    "\n",
    "            loss, recon_loss, kl_loss = loss_function(recon_images, images, mu, logvar)\n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_kl_loss += kl_loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    avg_recon_loss = total_recon_loss / len(dataloader.dataset)\n",
    "    avg_kl_loss = total_kl_loss / len(dataloader.dataset)\n",
    "    \n",
    "    print(f\"Evaluation Loss: {avg_loss:.4f}, Recon Loss: {avg_recon_loss:.4f}, KL Loss: {avg_kl_loss:.4f}\")\n",
    "    return avg_loss, avg_recon_loss, avg_kl_loss\n",
    "\n",
    "def plot_vae_reconstructions(model, dataloader, device, n=6):\n",
    "    \"\"\"Plot original and reconstructed images from the VAE.\"\"\"\n",
    "    model.eval()\n",
    "    images, _ = next(iter(dataloader))\n",
    "    indices = torch.randperm(images.size(0))[:n]\n",
    "    images = images[indices].to(device)\n",
    "    with torch.no_grad():\n",
    "        recon_images, _, _ = model(images)\n",
    "\n",
    "    # Plot original and reconstructed images\n",
    "    fig, axs = plt.subplots(2, n, figsize=(n * 2, 4))\n",
    "    for i in range(n):\n",
    "        # Original images\n",
    "        axs[0, i].imshow((images[i].permute(1, 2, 0).cpu().numpy() + 1) / 2)\n",
    "        axs[0, i].set_title(\"Original\")\n",
    "        axs[0, i].axis(\"off\")\n",
    "        # Reconstructed images\n",
    "        axs[1, i].imshow((recon_images[i].permute(1, 2, 0).cpu().numpy() + 1) / 2)\n",
    "        axs[1, i].set_title(\"Reconstructed\")\n",
    "        axs[1, i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainingsmethode definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "def train_vae(model, train_loader, val_loader, optimizer, vae_config, wandb_mode='online'):\n",
    "    \"\"\"Train the VAE model with wandb logging and evaluation every 5 epochs.\"\"\"\n",
    "    wandb.init(\n",
    "        project=\"vae-celeba\",\n",
    "        config=vae_config,\n",
    "        mode=wandb_mode\n",
    "    )\n",
    "\n",
    "    for epoch in range(vae_config[\"epochs\"]):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        total_recon_loss = 0\n",
    "        total_kl_loss = 0\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            recon_images, mu, logvar = model(images)\n",
    "\n",
    "            loss, recon_loss, kl_loss = loss_function(recon_images, images, mu, logvar)\n",
    "            loss.backward()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_kl_loss += kl_loss.item()\n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_loss = train_loss / len(train_loader.dataset)\n",
    "        avg_recon_loss = total_recon_loss / len(train_loader.dataset)\n",
    "        avg_kl_loss = total_kl_loss / len(train_loader.dataset)\n",
    "        \n",
    "        wandb.log({\"epoch\": epoch + 1, \"train/loss\": avg_loss, \"train/recon_loss\": avg_recon_loss, \"train/kl_loss\": avg_kl_loss})\n",
    "        print(f\"Epoch {epoch + 1}/{vae_config['epochs']}, Train Loss: {avg_loss:.4f}, Recon Loss: {avg_recon_loss:.4f}, KL Loss: {avg_kl_loss:.4f}\")\n",
    "\n",
    "        # Evaluate and log every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            val_loss, avg_recon_loss, avg_kl_loss = evaluate_vae(model, val_loader, device)\n",
    "            wandb.log({\"epoch\": epoch + 1, \"val/loss\": val_loss, \"val/avg_recon_loss\": avg_recon_loss, \"val/avg_kl_loss\": avg_kl_loss})\n",
    "\n",
    "            fig = plot_vae_reconstructions(model, val_loader, device, n=6)\n",
    "            plt.show()\n",
    "            wandb.log({\"val/reconstruction_images\": wandb.Image(fig)})\n",
    "            \n",
    "            plt.close(fig)\n",
    "\n",
    "    wandb.finish()\n",
    "    return model, avg_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainiere oder lade das Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the model save path\n",
    "vae_model_dir = \"./models/vae-celeba\"\n",
    "vae_model_path = Path(vae_model_dir) / f\"vae_celeba_latent_{vae_config['latent_dim']}_epochs_{vae_config['epochs']}_batch_{vae_config['batch_size']}_subset_{vae_config['subset_size']}.pth\"\n",
    "\n",
    "# Check if the model file exists\n",
    "if vae_model_path.exists():\n",
    "    print(\"VAE model file found. Loading the model...\")\n",
    "    model.load_state_dict(torch.load(vae_model_path))\n",
    "    model.eval()\n",
    "    fig = plot_vae_reconstructions(model, celeba_val_loader, device, n=6)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"VAE model file not found. Training the model...\")\n",
    "    # Train the model\n",
    "    model, avg_train_loss, avg_val_loss = train_vae(model, celeba_train_loader, celeba_val_loader, optimizer, vae_config, wandb_mode='disabled')\n",
    "    print(f\"Training completed. Average training loss: {avg_train_loss}, Average validation loss: {avg_val_loss}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    vae_model_dir_path = Path(vae_model_dir)\n",
    "    vae_model_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model.state_dict(), vae_model_path)\n",
    "    print(f\"VAE model saved to {vae_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Space: Manipulation und Interpolation\n",
    "\n",
    "Nachdem unser VAE trainiert ist, demonstrieren wir die Interpolation im latenten Raum:\n",
    "1. Zwei Bilder wählen  \n",
    "2. Latente Vektoren extrahieren  \n",
    "3. Dazwischen interpolieren  \n",
    "4. Die Zwischenschritte dekodieren und visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(z1, z2, num_steps=8):\n",
    "    \"\"\"Generate a series of interpolated latent vectors between z1 and z2.\"\"\"\n",
    "    ratios = np.linspace(0, 1, num_steps)\n",
    "    interpolated = [ (1 - r) * z1 + r * z2 for r in ratios ]\n",
    "    return torch.stack(interpolated)\n",
    "\n",
    "# Get two images from the dataset\n",
    "data_iter = iter(celeba_val_loader)\n",
    "images, _ = next(data_iter)\n",
    "img1 = images[0].unsqueeze(0).to(device)\n",
    "img2 = images[1].unsqueeze(0).to(device)\n",
    "\n",
    "# Encode images\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mu1, _ = model.encode(img1)\n",
    "    mu2, _ = model.encode(img2)\n",
    "\n",
    "# Interpolate in the latent space\n",
    "num_steps = 8\n",
    "interpolated_z = interpolate(mu1, mu2, num_steps=num_steps)\n",
    "\n",
    "# Decode the interpolated latent vectors\n",
    "with torch.no_grad():\n",
    "    decoded_imgs = model.decode(interpolated_z).cpu()\n",
    "\n",
    "# Plot the interpolation results\n",
    "fig, axes = plt.subplots(1, num_steps, figsize=(20, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    # Denormalize image from [-1,1] to [0,1]\n",
    "    img = (decoded_imgs[i].permute(1, 2, 0).numpy() + 1) / 2.0\n",
    "    ax.imshow(np.clip(img, 0, 1))\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Latent Space Interpolation between Two CelebA Images\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CelebA Attribute (Indizes binärer Labels)\n",
    "\n",
    "Wir können gezielt Attribute im latenten Raum manipulieren — z. B. eine nicht-lächelnde Person zum Lächeln bringen. Die CelebA-Daten enthalten binäre Attribute (Labels), die solche Eigenschaften markieren.\n",
    "\n",
    "\n",
    "| **Index** | **Attribute**         | **Index** | **Attribute**         | **Index** | **Attribute**         |\n",
    "|-----------|------------------------|-----------|------------------------|-----------|------------------------|\n",
    "| **0**     | 5_o_Clock_Shadow       | **14**    | Double_Chin           | **28**    | Receding_Hairline     |\n",
    "| **1**     | Arched_Eyebrows        | **15**    | Eyeglasses            | **29**    | Rosy_Cheeks           |\n",
    "| **2**     | Attractive             | **16**    | Goatee                | **30**    | Sideburns             |\n",
    "| **3**     | Bags_Under_Eyes        | **17**    | Gray_Hair             | **31**    | Smiling               |\n",
    "| **4**     | Bald                   | **18**    | Heavy_Makeup          | **32**    | Straight_Hair         |\n",
    "| **5**     | Bangs                  | **19**    | High_Cheekbones       | **33**    | Wavy_Hair             |\n",
    "| **6**     | Big_Lips               | **20**    | Male                  | **34**    | Wearing_Earrings      |\n",
    "| **7**     | Big_Nose               | **21**    | Mouth_Slightly_Open   | **35**    | Wearing_Hat           |\n",
    "| **8**     | Black_Hair             | **22**    | Mustache              | **36**    | Wearing_Lipstick      |\n",
    "| **9**     | Blond_Hair             | **23**    | Narrow_Eyes           | **37**    | Wearing_Necklace      |\n",
    "| **10**    | Blurry                 | **24**    | No_Beard              | **38**    | Wearing_Necktie       |\n",
    "| **11**    | Brown_Hair             | **25**    | Oval_Face             | **39**    | Young                 |\n",
    "| **12**    | Bushy_Eyebrows         | **26**    | Pale_Skin             |           |                        |\n",
    "| **13**    | Chubby                 | **27**    | Pointy_Nose           |           |                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Funktion gibt die latenten Vektoren und die Werte eines bestimmten Attributs zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latents_and_attrs(model, dataloader, attribute_idx):\n",
    "    \"\"\"Get latent vectors and corresponding attributes from the dataset.\"\"\"\n",
    "    x, a = next(iter(dataloader))\n",
    "    x = x.to(device)\n",
    "    with torch.no_grad():\n",
    "        mu, _ = model.encode(x)\n",
    "    latents = mu.cpu()\n",
    "    attrs = a[:, attribute_idx].cpu()\n",
    "    return latents, attrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden manipulieren wir gezielt den latenten Raum anhand eines Attributes, indem wir in die Richtung des Merkmals interpolieren. So wird z. B. ein nicht-lächelndes Gesicht schrittweise zu einem lächelnden Bild transformiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attribute_interpolation(model, latents, attrs, num_steps=8, device='cuda'):\n",
    "    \"\"\"\n",
    "    Visualize the interpolation of a specific attribute in the latent space.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained VAE model.\n",
    "        latents (torch.Tensor): Latent representations of the dataset.\n",
    "        attrs (torch.Tensor): Attribute labels corresponding to the dataset.\n",
    "        attribute_idx (int): Index of the attribute to visualize.\n",
    "        num_steps (int): Number of interpolation steps. Default is 8.\n",
    "        device (str): Device to use ('cuda' or 'cpu'). Default is 'cuda'.\n",
    "    \"\"\"\n",
    "    # Compute the direction vector for the attribute\n",
    "    attribute_mean_positive = latents[attrs == 1].mean(dim=0)\n",
    "    attribute_mean_negative = latents[attrs == 0].mean(dim=0)\n",
    "    direction = attribute_mean_positive - attribute_mean_negative\n",
    "\n",
    "    # Pick a random sample with the negative attribute\n",
    "    negative_attribute_latent = latents[attrs == 0][10].unsqueeze(0)\n",
    "\n",
    "    # Interpolate between the negative attribute and the positive attribute\n",
    "    interpolated_latents = torch.stack([\n",
    "        negative_attribute_latent + alpha * direction\n",
    "        for alpha in torch.linspace(0, 1, num_steps)\n",
    "    ]).to(device)\n",
    "\n",
    "    # Decode the interpolated latent vectors\n",
    "    with torch.no_grad():\n",
    "        interpolated_faces = model.decode(interpolated_latents).cpu()\n",
    "\n",
    "    # Plot the interpolation results\n",
    "    fig, axes = plt.subplots(1, num_steps, figsize=(20, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow((interpolated_faces[i].squeeze().permute(1, 2, 0).numpy() + 1) / 2)\n",
    "        ax.set_title(f\"Step {i + 1}\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "attribute_idx = 31\n",
    "\n",
    "latents, attrs = get_latents_and_attrs(model, celeba_val_loader, attribute_idx)\n",
    "visualize_attribute_interpolation(model, latents, attrs, num_steps=8, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können auch neue Gesichter erzeugen, indem wir Zufallsvektoren im latenten Raum samplen und durch den Decoder schicken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_vae(model, latent_dim=200, num_samples=8, device='cuda'):\n",
    "    \"\"\"\n",
    "    Sample random latent vectors from a standard Gaussian,\n",
    "    decode them using the VAE decoder, and visualize the generated images.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        generated = model.decode(z).cpu()\n",
    "\n",
    "    # Plot the generated images\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 2, 2))\n",
    "    for i in range(num_samples):\n",
    "        img = (generated[i].permute(1, 2, 0).numpy() + 1) / 2.0  # denormalize from [-1, 1] to [0, 1]\n",
    "        axes[i].imshow(np.clip(img, 0, 1))\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.suptitle(\"Random Samples from VAE Latent Space\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run it\n",
    "sample_from_vae(model, latent_dim=vae_config[\"latent_dim\"], device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Übung\n",
    "\n",
    "Trainiere den VAE auf dem MNIST-Datensatz und vergleiche die Ergebnisse mit dem klassischen Autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Empfohlene Ressourcen:**\n",
    "- [Was ist ein latenter Raum? (IBM)](https://www.ibm.com/think/topics/latent-space)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
