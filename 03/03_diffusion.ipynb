{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Models: A Comprehensive Notebook\n",
    "\n",
    "This notebook demonstrates a simplified implementation of a diffusion model (a DDPM – Denoising Diffusion Probabilistic Model) using PyTorch. We will:\n",
    "\n",
    "- Introduce the forward (noise-adding) and reverse (denoising) diffusion processes.\n",
    "- Build a simple convolutional model that learns to predict the noise added to images.\n",
    "- Train the model on MNIST (for demonstration purposes) while logging training metrics with wandb.\n",
    "- Sample images from the learned model and visualize intermediate steps using Matplotlib and Seaborn.\n",
    "\n",
    "**Note:** Training full diffusion models (e.g., on high-resolution datasets) requires significant compute. This notebook is meant for educational purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The U-Net is a deep neural network architecture originally developed for biomedical image segmentation. It has since become a fundamental building block in generative AI systems, notably in diffusion models.\n",
    "\n",
    "The architecture gets its name from its distinctive \"U\"-shaped structure, composed of two main parts:\n",
    "\n",
    "- **Encoder (Contracting path):** Gradually downsamples the input, capturing context and reducing spatial dimensions. This phase extracts high-level features.\n",
    "\n",
    "- **Decoder (Expanding path):** Gradually upsamples the encoded features to restore the original spatial dimensions, enabling precise spatial localization.\n",
    "\n",
    "At the deepest point, known as the bottleneck, the representation captures highly abstracted, compressed information.\n",
    "\n",
    "This encoder-decoder structure allows U-Net to efficiently learn and represent complex patterns, making it ideal for tasks such as image generation, denoising, segmentation, and other spatially-sensitive problems.\n",
    "\n",
    "A key innovation in U-Net is the use of **skip connections**—direct links that transfer feature maps from layers in the encoder directly to corresponding layers in the decoder. \n",
    "\n",
    "In practice, skip connections significantly enhance the quality and sharpness of the generated outputs, which is critical for generative models, including diffusion models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Minimal Denoising U-Net\n",
    "\n",
    "In this section, we'll build a minimal yet functional U-Net architecture step-by-step. We'll then train it for a simple image-denoising task using the Fashion-MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Design\n",
    "\n",
    "The U-Net has two encoder blocks, a botter neck layer, and two decoder blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.enc1 = nn.Sequential(nn.Conv2d(1, 16, 3, padding=1), nn.ReLU())\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc2 = nn.Sequential(nn.Conv2d(16, 32, 3, padding=1), nn.ReLU())\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(nn.Conv2d(32, 64, 3, padding=1), nn.ReLU())\n",
    "\n",
    "        # Decoder layers\n",
    "        self.upconv2 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        self.dec2 = nn.Sequential(nn.Conv2d(64, 32, 3, padding=1), nn.ReLU())\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(32, 16, 2, stride=2)\n",
    "        self.dec1 = nn.Sequential(nn.Conv2d(32, 16, 3, padding=1), nn.ReLU())\n",
    "\n",
    "        # Final Convolution\n",
    "        self.conv_final = nn.Conv2d(16, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool1(enc1))\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool2(enc2))\n",
    "\n",
    "        # Decoder\n",
    "        dec2 = self.upconv2(bottleneck)\n",
    "        dec2 = self.dec2(torch.cat((dec2, enc2), dim=1))\n",
    "\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = self.dec1(torch.cat((dec1, enc1), dim=1))\n",
    "\n",
    "        return self.conv_final(dec1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Fashion-MNIST Dataset for Denoising Task\n",
    "\n",
    "We'll add artificial Gaussian noise to the images and train the model to denoise them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.FashionMNIST(root='../data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "def add_noise(img, noise_factor=0.5):\n",
    "    noisy_img = img + noise_factor * torch.randn_like(img)\n",
    "    return torch.clip(noisy_img, 0., 1.)\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "images, _ = next(data_iter)\n",
    "noisy_images = add_noise(images)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(images[0].squeeze(), cmap='gray')\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[1].imshow(noisy_images[0].squeeze(), cmap='gray')\n",
    "axes[1].set_title(\"Noisy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The Network\n",
    "\n",
    "We will train the network for five epochs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        noisy_images = add_noise(images).to(device)\n",
    "\n",
    "        outputs = model(noisy_images)\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_data = datasets.FashionMNIST(root='../data', train=False, download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "    for images, _ in test_loader:\n",
    "        images = images.to(device)\n",
    "        noisy_images = add_noise(images).to(device)\n",
    "\n",
    "        outputs = model(noisy_images)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3)\n",
    "        axes[0].imshow(images[0].cpu().numpy().squeeze(), cmap='gray')\n",
    "        axes[0].set_title(\"Original\")\n",
    "        axes[1].imshow(noisy_images[0].cpu().numpy().squeeze(), cmap='gray')\n",
    "        axes[1].set_title(\"Noisy\")\n",
    "        axes[2].imshow(outputs[0].cpu().numpy().squeeze(), cmap='gray')\n",
    "        axes[2].set_title(\"Denoised\")\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Initialization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Initialize wandb (make sure you are logged in to wandb)\n",
    "wandb.init(project=\"diffusion-model-demo\", mode=\"disabled\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We will use the MNIST dataset for demonstration. Images are scaled to [0,1] and converted to tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation: MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts image to tensor (C x H x W) in [0,1]\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "We define a simple CNN that takes as input an image along with a time-step embedding. This model will learn to predict the noise that was added during the forward diffusion process.\n",
    "\n",
    "*Note: In practice, diffusion models use U-Net architectures with skip connections; here, we use a simplified architecture for clarity.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple time embedding module and CNN for noise prediction\n",
    "class SimpleDiffusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleDiffusionModel, self).__init__()\n",
    "        # Time embedding: embed a scalar time into a vector and reshape for broadcasting\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28 * 28)\n",
    "        )\n",
    "        # A simple convolutional network to predict noise\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # x: (batch, 1, 28, 28), t: (batch,) time steps\n",
    "        # Embed t and reshape to image size\n",
    "        t = t.float().unsqueeze(1)  # shape (batch, 1)\n",
    "        t_emb = self.time_embed(t)  # shape (batch, 28*28)\n",
    "        t_emb = t_emb.view(-1, 1, 28, 28)\n",
    "        # Combine image and time embedding (here, simply add them)\n",
    "        x = x + t_emb\n",
    "        return self.conv(x)\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleDiffusionModel().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Process Setup\n",
    "\n",
    "We now define the forward diffusion (adding noise) schedule. In a diffusion model, noise is added gradually over T timesteps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for diffusion\n",
    "T = 100  # total diffusion steps (for demonstration; real models use many more)\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "\n",
    "# Linear beta schedule\n",
    "betas = torch.linspace(beta_start, beta_end, T).to(device)  # shape (T,)\n",
    "alphas = 1 - betas\n",
    "alphas_hat = torch.cumprod(alphas, dim=0)  # cumulative product for each timestep\n",
    "\n",
    "# Helper function: forward diffusion (q_sample)\n",
    "def q_sample(x0, t, noise=None):\n",
    "    \"\"\"\n",
    "    Diffuse the data (add noise) for a given timestep.\n",
    "    x0: original image, shape (batch, 1, 28, 28)\n",
    "    t: timestep tensor, shape (batch,)\n",
    "    noise: optional noise tensor; if None, sampled from standard normal\n",
    "    \"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x0)\n",
    "    # Get corresponding alphas_hat for each timestep t (need to index properly)\n",
    "    # Expand dims so that we can multiply with x0\n",
    "    sqrt_alphas_hat = torch.sqrt(alphas_hat[t]).view(-1, 1, 1, 1)\n",
    "    sqrt_one_minus_alphas_hat = torch.sqrt(1 - alphas_hat[t]).view(-1, 1, 1, 1)\n",
    "    return sqrt_alphas_hat * x0 + sqrt_one_minus_alphas_hat * noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "For each training step:\n",
    "- Sample a batch of images.\n",
    "- Randomly choose a timestep \\( t \\) for each image.\n",
    "- Compute the noised image using our forward diffusion function.\n",
    "- Have the model predict the noise given the noised image and the timestep.\n",
    "- Compute the loss (MSE) between the true noise and the model’s prediction.\n",
    "- Log training loss to wandb.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 5  # For demonstration, keep epochs small\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch_idx, (x0, _) in enumerate(train_loader):\n",
    "        x0 = x0.to(device)  # shape (batch, 1, 28, 28)\n",
    "        batch_size_current = x0.size(0)\n",
    "        \n",
    "        # Sample a random timestep for each image in the batch\n",
    "        t = torch.randint(0, T, (batch_size_current,), device=device)\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(x0)\n",
    "        # Generate noised image at timestep t\n",
    "        x_noisy = q_sample(x0, t, noise)\n",
    "        \n",
    "        # Predict the noise using our model\n",
    "        noise_pred = model(x_noisy, t)\n",
    "        \n",
    "        # Compute loss: Mean Squared Error between actual noise and predicted noise\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "        \n",
    "        # For demonstration, log one batch every 500 iterations\n",
    "        if batch_idx % 500 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling: Reverse Diffusion Process\n",
    "\n",
    "After training, we can sample new images by starting from pure noise and iteratively denoising. The reverse update (for a given timestep \\( t \\)) is approximated by:\n",
    "  \n",
    "\\[\n",
    "x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_\\theta(x_t, t) \\right) + \\sigma_t z\n",
    "\\]\n",
    "  \n",
    "For simplicity, our implementation uses a basic version without elaborate noise scheduling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_image(model, image_size=28):\n",
    "    # Start from random noise\n",
    "    x = torch.randn(1, 1, image_size, image_size).to(device)\n",
    "    for t in reversed(range(T)):\n",
    "        t_tensor = torch.full((1,), t, device=device, dtype=torch.long)\n",
    "        beta = betas[t]\n",
    "        alpha = alphas[t]\n",
    "        alpha_hat_t = alphas_hat[t]\n",
    "        \n",
    "        # Predict noise at current step\n",
    "        pred_noise = model(x, t_tensor)\n",
    "        \n",
    "        # Compute the reverse update (simplified version)\n",
    "        x = (1 / torch.sqrt(alpha)) * (x - (beta / torch.sqrt(1 - alpha_hat_t)) * pred_noise)\n",
    "        \n",
    "        # For t > 0, add noise\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            x = x + torch.sqrt(beta) * noise\n",
    "    # Clamp values to [0,1] for visualization\n",
    "    return torch.clamp(x, 0., 1.)\n",
    "\n",
    "# Generate a sample image\n",
    "sampled_img = sample_image(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We use Matplotlib and Seaborn to visualize the generated image and plot training loss curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a generated sample image\n",
    "def show_image(img_tensor, title=\"Generated Image\"):\n",
    "    img = img_tensor.squeeze().cpu().numpy()\n",
    "    plt.figure(figsize=(3,3))\n",
    "    sns.heatmap(img, cmap=\"gray\", cbar=False, xticklabels=False, yticklabels=False)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_image(sampled_img, title=\"Sampled Image from Diffusion Model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "- Introduced the basics of diffusion models.\n",
    "- Built a simplified diffusion model on MNIST.\n",
    "- Implemented the forward diffusion (noise addition) and reverse (denoising/sampling) processes.\n",
    "- Trained the model while tracking loss with wandb.\n",
    "- Sampled and visualized new images from our model.\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with more complex architectures (e.g., U-Net) and datasets (CIFAR-10, CelebA).\n",
    "- Explore conditional diffusion models and latent diffusion methods.\n",
    "- Fine-tune hyperparameters and training strategies for improved sample quality.\n",
    "- Review state-of-the-art implementations using the Hugging Face diffusers library.\n",
    "\n",
    "Happy experimenting!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
