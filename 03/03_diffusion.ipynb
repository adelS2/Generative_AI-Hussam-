{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Models: A Comprehensive Notebook\n",
    "\n",
    "This notebook demonstrates a simplified implementation of a diffusion model (a DDPM – Denoising Diffusion Probabilistic Model) using PyTorch. We will:\n",
    "\n",
    "- Introduce the forward (noise-adding) and reverse (denoising) diffusion processes.\n",
    "- Build a simple convolutional model that learns to predict the noise added to images.\n",
    "- Train the model on MNIST (for demonstration purposes) while logging training metrics with wandb.\n",
    "- Sample images from the learned model and visualize intermediate steps using Matplotlib and Seaborn.\n",
    "\n",
    "**Note:** Training full diffusion models (e.g., on high-resolution datasets) requires significant compute. This notebook is meant for educational purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports and Initialization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Initialize wandb (make sure you are logged in to wandb)\n",
    "wandb.init(project=\"diffusion-model-demo\", mode=\"disabled\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We will use the MNIST dataset for demonstration. Images are scaled to [0,1] and converted to tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation: MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts image to tensor (C x H x W) in [0,1]\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "We define a simple CNN that takes as input an image along with a time-step embedding. This model will learn to predict the noise that was added during the forward diffusion process.\n",
    "\n",
    "*Note: In practice, diffusion models use U-Net architectures with skip connections; here, we use a simplified architecture for clarity.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple time embedding module and CNN for noise prediction\n",
    "class SimpleDiffusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleDiffusionModel, self).__init__()\n",
    "        # Time embedding: embed a scalar time into a vector and reshape for broadcasting\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28 * 28)\n",
    "        )\n",
    "        # A simple convolutional network to predict noise\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # x: (batch, 1, 28, 28), t: (batch,) time steps\n",
    "        # Embed t and reshape to image size\n",
    "        t = t.float().unsqueeze(1)  # shape (batch, 1)\n",
    "        t_emb = self.time_embed(t)  # shape (batch, 28*28)\n",
    "        t_emb = t_emb.view(-1, 1, 28, 28)\n",
    "        # Combine image and time embedding (here, simply add them)\n",
    "        x = x + t_emb\n",
    "        return self.conv(x)\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleDiffusionModel().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Process Setup\n",
    "\n",
    "We now define the forward diffusion (adding noise) schedule. In a diffusion model, noise is added gradually over T timesteps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for diffusion\n",
    "T = 100  # total diffusion steps (for demonstration; real models use many more)\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "\n",
    "# Linear beta schedule\n",
    "betas = torch.linspace(beta_start, beta_end, T).to(device)  # shape (T,)\n",
    "alphas = 1 - betas\n",
    "alphas_hat = torch.cumprod(alphas, dim=0)  # cumulative product for each timestep\n",
    "\n",
    "# Helper function: forward diffusion (q_sample)\n",
    "def q_sample(x0, t, noise=None):\n",
    "    \"\"\"\n",
    "    Diffuse the data (add noise) for a given timestep.\n",
    "    x0: original image, shape (batch, 1, 28, 28)\n",
    "    t: timestep tensor, shape (batch,)\n",
    "    noise: optional noise tensor; if None, sampled from standard normal\n",
    "    \"\"\"\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x0)\n",
    "    # Get corresponding alphas_hat for each timestep t (need to index properly)\n",
    "    # Expand dims so that we can multiply with x0\n",
    "    sqrt_alphas_hat = torch.sqrt(alphas_hat[t]).view(-1, 1, 1, 1)\n",
    "    sqrt_one_minus_alphas_hat = torch.sqrt(1 - alphas_hat[t]).view(-1, 1, 1, 1)\n",
    "    return sqrt_alphas_hat * x0 + sqrt_one_minus_alphas_hat * noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "For each training step:\n",
    "- Sample a batch of images.\n",
    "- Randomly choose a timestep \\( t \\) for each image.\n",
    "- Compute the noised image using our forward diffusion function.\n",
    "- Have the model predict the noise given the noised image and the timestep.\n",
    "- Compute the loss (MSE) between the true noise and the model’s prediction.\n",
    "- Log training loss to wandb.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 5  # For demonstration, keep epochs small\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch_idx, (x0, _) in enumerate(train_loader):\n",
    "        x0 = x0.to(device)  # shape (batch, 1, 28, 28)\n",
    "        batch_size_current = x0.size(0)\n",
    "        \n",
    "        # Sample a random timestep for each image in the batch\n",
    "        t = torch.randint(0, T, (batch_size_current,), device=device)\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(x0)\n",
    "        # Generate noised image at timestep t\n",
    "        x_noisy = q_sample(x0, t, noise)\n",
    "        \n",
    "        # Predict the noise using our model\n",
    "        noise_pred = model(x_noisy, t)\n",
    "        \n",
    "        # Compute loss: Mean Squared Error between actual noise and predicted noise\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "        \n",
    "        # For demonstration, log one batch every 500 iterations\n",
    "        if batch_idx % 500 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling: Reverse Diffusion Process\n",
    "\n",
    "After training, we can sample new images by starting from pure noise and iteratively denoising. The reverse update (for a given timestep \\( t \\)) is approximated by:\n",
    "  \n",
    "\\[\n",
    "x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\hat{\\epsilon}_\\theta(x_t, t) \\right) + \\sigma_t z\n",
    "\\]\n",
    "  \n",
    "For simplicity, our implementation uses a basic version without elaborate noise scheduling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_image(model, image_size=28):\n",
    "    # Start from random noise\n",
    "    x = torch.randn(1, 1, image_size, image_size).to(device)\n",
    "    for t in reversed(range(T)):\n",
    "        t_tensor = torch.full((1,), t, device=device, dtype=torch.long)\n",
    "        beta = betas[t]\n",
    "        alpha = alphas[t]\n",
    "        alpha_hat_t = alphas_hat[t]\n",
    "        \n",
    "        # Predict noise at current step\n",
    "        pred_noise = model(x, t_tensor)\n",
    "        \n",
    "        # Compute the reverse update (simplified version)\n",
    "        x = (1 / torch.sqrt(alpha)) * (x - (beta / torch.sqrt(1 - alpha_hat_t)) * pred_noise)\n",
    "        \n",
    "        # For t > 0, add noise\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            x = x + torch.sqrt(beta) * noise\n",
    "    # Clamp values to [0,1] for visualization\n",
    "    return torch.clamp(x, 0., 1.)\n",
    "\n",
    "# Generate a sample image\n",
    "sampled_img = sample_image(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We use Matplotlib and Seaborn to visualize the generated image and plot training loss curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a generated sample image\n",
    "def show_image(img_tensor, title=\"Generated Image\"):\n",
    "    img = img_tensor.squeeze().cpu().numpy()\n",
    "    plt.figure(figsize=(3,3))\n",
    "    sns.heatmap(img, cmap=\"gray\", cbar=False, xticklabels=False, yticklabels=False)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_image(sampled_img, title=\"Sampled Image from Diffusion Model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "- Introduced the basics of diffusion models.\n",
    "- Built a simplified diffusion model on MNIST.\n",
    "- Implemented the forward diffusion (noise addition) and reverse (denoising/sampling) processes.\n",
    "- Trained the model while tracking loss with wandb.\n",
    "- Sampled and visualized new images from our model.\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with more complex architectures (e.g., U-Net) and datasets (CIFAR-10, CelebA).\n",
    "- Explore conditional diffusion models and latent diffusion methods.\n",
    "- Fine-tune hyperparameters and training strategies for improved sample quality.\n",
    "- Review state-of-the-art implementations using the Hugging Face diffusers library.\n",
    "\n",
    "Happy experimenting!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
