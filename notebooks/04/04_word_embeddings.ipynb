{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fd4ff3f",
   "metadata": {},
   "source": [
    "# Tokens, Word Embeddings & Language Representation\n",
    "\n",
    "## Learning Objectives:\n",
    "- Understand what tokens are.\n",
    "- Learn how to tokenize text using the Hugging Face Transformers library.\n",
    "- Explore what embeddings are, their role in representing meaning, and how to extract them from a pretrained neural network model.\n",
    "- Work with a real dataset to see these ideas in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60f2a27",
   "metadata": {},
   "source": [
    "## Introduction to Tokens\n",
    "\n",
    "In Natural Language Processing (NLP), **tokens** are the basic units into which text is split. They can be words, subwords, or characters. Tokenization is essential because:\n",
    "- **Standardization:** It converts raw text into a standardized form.\n",
    "- **Input for Models:** Neural networks process fixed units (tokens), not raw text.\n",
    "- **Handling Vocabulary:** It helps in building a vocabulary and managing out-of-vocabulary words.\n",
    "\n",
    "Below, we will see how to tokenize text using a pretrained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ef07cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " Generative AI is transforming the way we work with data.\n",
      "\n",
      "Tokens:\n",
      " ['genera', '##tive', 'ai', 'is', 'transforming', 'the', 'way', 'we', 'work', 'with', 'data', '.']\n",
      "\n",
      "Token IDs:\n",
      " [101, 11416, 6024, 9932, 2003, 17903, 1996, 2126, 2057, 2147, 2007, 2951, 1012, 102]\n",
      "\n",
      "Decoded Text:\n",
      " [CLS] generative ai is transforming the way we work with data. [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a pretrained tokenizer (we'll use 'bert-base-uncased' as an example).\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example text for tokenization\n",
    "example_text = \"Generative AI is transforming the way we work with data.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(example_text)\n",
    "token_ids = tokenizer(example_text)[\"input_ids\"]\n",
    "\n",
    "print(\"Original Text:\\n\", example_text)\n",
    "print(\"\\nTokens:\\n\", tokens)\n",
    "print(\"\\nToken IDs:\\n\", token_ids)\n",
    "print(\"\\nDecoded Text:\\n\", tokenizer.decode(token_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7459b0",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### Main Tasks of a Tokenizer\n",
    "\n",
    "Tokenizers can be thought of as **lookup tables** that map text to numerical representations, enabling models to process and understand language.\n",
    "\n",
    "1. **Tokenization**: Splits text into tokens (e.g., words, subwords, or characters).\n",
    "2. **Token-to-ID Mapping**: Converts tokens into numerical IDs.\n",
    "3. **Special Tokens**: Adds tokens like `[CLS]` or `[SEP]` for specific models.\n",
    "4. **OOV Handling**: Replaces unknown words with `[UNK]`.\n",
    "5. **Padding/Truncation**: Adjusts sequence lengths with `[PAD]` or truncates.\n",
    "6. **Attention Masks**: Marks real tokens vs. padding (e.g., `[1, 1, 0, 0]`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76daa3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 30522\n",
      "\n",
      "Sample Vocabulary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>artisans</td>\n",
       "      <td>26818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>duration</td>\n",
       "      <td>9367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>euclidean</td>\n",
       "      <td>25826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ind</td>\n",
       "      <td>27427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sbs</td>\n",
       "      <td>21342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alloys</td>\n",
       "      <td>28655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>realization</td>\n",
       "      <td>12393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>##emia</td>\n",
       "      <td>17577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>employed</td>\n",
       "      <td>4846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bahamas</td>\n",
       "      <td>17094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Token  Index\n",
       "0     artisans  26818\n",
       "1     duration   9367\n",
       "2    euclidean  25826\n",
       "3          ind  27427\n",
       "4          sbs  21342\n",
       "5       alloys  28655\n",
       "6  realization  12393\n",
       "7       ##emia  17577\n",
       "8     employed   4846\n",
       "9      bahamas  17094"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the vocabulary\n",
    "vocabulary = tokenizer.vocab\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the vocabulary dictionary to a DataFrame\n",
    "vocab_df = pd.DataFrame(list(vocabulary.items()), columns=[\"Token\", \"Index\"])\n",
    "\n",
    "# Display the first 20 tokens in a tabular format\n",
    "print(\"Vocabulary Size:\", len(vocabulary))\n",
    "print(\"\\nSample Vocabulary:\")\n",
    "vocab_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9f2242",
   "metadata": {},
   "source": [
    "## Working with a Real Dataset\n",
    "\n",
    "For practical learning, we use a real dataset from Hugging Face. In this notebook, we load a small subset of the IMDb dataset, which is widely used for NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "164efafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Review:\n",
      " I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attent ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a small subset of the IMDb dataset\n",
    "dataset = load_dataset(\"imdb\", split=\"train[:1000]\")\n",
    "\n",
    "# Inspect a sample review\n",
    "sample_review = dataset[0][\"text\"]\n",
    "print(\"Sample Review:\\n\", sample_review[:500], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd662e18",
   "metadata": {},
   "source": [
    "We can tokenize the entire dataset efficiently in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54eca91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>[101, 1045, 12524, 1045, 2572, 8025, 1011, 375...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>[101, 1000, 1045, 2572, 8025, 1024, 3756, 1000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>[101, 2065, 2069, 2000, 4468, 2437, 2023, 2828...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>[101, 2023, 2143, 2001, 2763, 4427, 2011, 2643...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>[101, 2821, 1010, 2567, 1012, 1012, 1012, 2044...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...   \n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...   \n",
       "2  If only to avoid making this type of film in t...   \n",
       "3  This film was probably inspired by Godard's Ma...   \n",
       "4  Oh, brother...after hearing about this ridicul...   \n",
       "\n",
       "                                           input_ids  \n",
       "0  [101, 1045, 12524, 1045, 2572, 8025, 1011, 375...  \n",
       "1  [101, 1000, 1045, 2572, 8025, 1024, 3756, 1000...  \n",
       "2  [101, 2065, 2069, 2000, 4468, 2437, 2023, 2828...  \n",
       "3  [101, 2023, 2143, 2001, 2763, 4427, 2011, 2643...  \n",
       "4  [101, 2821, 1010, 2567, 1012, 1012, 1012, 2044...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(lambda x: tokenizer(x['text'], padding='max_length', truncation=True), batched=True)\n",
    "\n",
    "# Inspect the tokenized dataset\n",
    "print(tokenized_dataset)\n",
    "\n",
    "# Convert the tokenized dataset to a pandas DataFrame\n",
    "tokenized_df = tokenized_dataset.to_pandas()[[\"text\", \"input_ids\"]]\n",
    "\n",
    "# Display the DataFrame\n",
    "tokenized_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
