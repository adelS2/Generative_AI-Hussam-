# 10-Day Practical Generative AI Course Plan

| **Day** | **Topics Covered** | **Practical Session/Activity** |
|---------|--------------------|--------------------------------|
| **Day 1** | **Introduction & Course Overview**<br>- What is Generative AI?<br>- Overview of VAEs, GANs, Transformers, LLMs, foundation models, etc.<br>- Ethical considerations and real-world applications | - Set up development environment (Python, PyTorch, libraries, API keys)<br>- Introduce the capstone project idea |
| **Day 2** | **Variational Autoencoders (VAEs)**<br>- Recap of autoencoders<br>- VAE fundamentals: latent variables, reparameterization trick, loss function (reconstruction + KL divergence) | - Build a VAE using PyTorch on MNIST<br>- Explore latent space interpolation and visualization |
| **Day 3** | **Generative Adversarial Networks (GANs) – Fundamentals**<br>- GAN architecture: generator vs. discriminator<br>- Loss functions and training challenges (mode collapse, instability) | - Code a basic GAN (e.g., DCGAN) in PyTorch to generate images<br>- Monitor training dynamics and discuss evaluation metrics |
| **Day 4** | **Advanced GANs & Variants**<br>- Overview of Conditional GANs, CycleGANs, StyleGAN basics<br>- Techniques for improved training stability and performance | - Implement a GAN variant (e.g., Conditional GAN) on a selected dataset<br>- Compare performance and discuss applications |
| **Day 5** | **Transformers – Architecture & Fundamentals**<br>- Understanding self-attention and the encoder–decoder model<br>- Comparison with RNNs/CNNs for sequence tasks | - Implement a simple Transformer model for a task (e.g., text classification or translation)<br>- Visualize and interpret attention weights |
| **Day 6** | **Large Language Models (LLMs) & Foundation Models**<br>- Overview of LLMs (GPT, BERT) and foundation models<br>- Pre-training, fine-tuning, and transfer learning concepts | - Fine-tune a pre-trained model (via Hugging Face) on a domain-specific dataset<br>- Experiment with text generation and analyze outputs |
| **Day 7** | **OpenAI API – Integration**<br>- Overview of OpenAI API offerings and endpoints<br>- Best practices for integrating cloud-based models | - Develop a mini-project (chatbot or text summarizer) using the OpenAI API<br>- Practice API integration and error handling |
| **Day 8** | **Hugging Face Ecosystem**<br>- Introduction to Hugging Face Transformers, Datasets, and Model Hub<br>- Leveraging community tools for generative tasks | - Use Hugging Face pipelines for text generation, classification, or translation<br>- Fine-tune a transformer model with the Hugging Face Trainer |
| **Day 9** | **Capstone Project – Bringing It All Together**<br>- Strategies for combining generative approaches<br>- Model deployment and scaling (containerization, cloud deployment) | - Begin development of an end-to-end generative AI application<br>- Team work, debugging, and peer feedback |
| **Day 10** | **Project Presentations & Future Trends**<br>- Student project presentations and feedback<br>- Recap of core concepts and discussion on emerging research trends | - Finalize and showcase the capstone project<br>- Engage in a Q&A session and receive pointers for further study |
